{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Buffer #################\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class MemoryBuffer:\n",
    "\n",
    "    def __init__(self, size):\n",
    "                    self.buffer = deque(maxlen=size)\n",
    "                    self.maxSize = size\n",
    "                    self.len = 0\n",
    "\n",
    "    def sample(self, count = 100):\n",
    "        \n",
    "        \"\"\"\n",
    "        samples a random batch from the replay memory buffer\n",
    "        :param count: batch size\n",
    "        :return: batch (numpy array)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count)\n",
    "\n",
    "        s_arr = np.array([arr[0] for arr in batch])\n",
    "        a_arr = np.array([arr[1] for arr in batch])\n",
    "        #print('\\nbatch in sampling',batch)#np.float32\n",
    "        r_arr = np.array([arr[2] for arr in batch])\n",
    "        s1_arr = np.array([arr[3] for arr in batch])\n",
    "        return s_arr, a_arr, r_arr, s1_arr\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def add(self, s, a, r, s1):\n",
    "        \"\"\"\n",
    "        adds a particular transaction in the memory buffer\n",
    "        :param s: current state\n",
    "        :param a: action taken\n",
    "        :param r: reward received\n",
    "        :param s1: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        transition = (s,a,r,s1)\n",
    "        self.len += 1\n",
    "        if self.len > self.maxSize:\n",
    "            self.len = self.maxSize\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Actor Critic Model ###############\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "EPS = 0.003\n",
    "\n",
    "def fanin_init(size, fanin=None):\n",
    "\tfanin = fanin or size[0]\n",
    "\tv = 1. / np.sqrt(fanin)\n",
    "\treturn torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "\tdef __init__(self, state_dim, action_dim):\n",
    "\t\t\"\"\"\n",
    "\t\t:param state_dim: Dimension of input state (int)\n",
    "\t\t:param action_dim: Dimension of input action (int)\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Critic, self).__init__()\n",
    "\n",
    "\t\tself.state_dim = state_dim\n",
    "\t\tself.action_dim = action_dim\n",
    "\n",
    "\t\tself.fcs1 = nn.Linear(state_dim,10)\n",
    "\t\tself.fcs1.weight.data = fanin_init(self.fcs1.weight.data.size())\n",
    "\t\tself.fcs1_bn = nn.BatchNorm1d(10)  \n",
    "        \n",
    "\t\tself.fcs2 = nn.Linear(10,16)\n",
    "\t\tself.fcs2.weight.data = fanin_init(self.fcs2.weight.data.size())\n",
    "\t\tself.fcs2_bn = nn.BatchNorm1d(16)\n",
    "        \n",
    "\t\tself.fca1 = nn.Linear(action_dim,16)\n",
    "\t\tself.fca1.weight.data = fanin_init(self.fca1.weight.data.size())\n",
    "\t\tself.fca1_bn = nn.BatchNorm1d(16)\n",
    "        \n",
    "\t\tself.fc2 = nn.Linear(10,16)\n",
    "\t\tself.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "\t\tself.fc2_bn = nn.BatchNorm1d(16)\n",
    "        \n",
    "\t\tself.fc3 = nn.Linear(16,1)\n",
    "\t\tself.fc3.weight.data.uniform_(-EPS,EPS)\n",
    "\n",
    "\tdef forward(self, state, action):\n",
    "\t\t\"\"\"\n",
    "\t\treturns Value function Q(s,a) obtained from critic network\n",
    "\t\t:param state: Input state (Torch Variable : [n,state_dim] )\n",
    "\t\t:param action: Input Action (Torch Variable : [n,action_dim] )\n",
    "\t\t:return: Value function : Q(S,a) (Torch Variable : [n,1] )\n",
    "\t\t\"\"\"\n",
    "\t\t#print(\"\\nsstate in before fwd\",state.size(),\"action in before fwd\",action.size())\n",
    "\t\ts1 = self.fcs1_bn(F.relu(self.fcs1(state)))\n",
    "\t\ts2 = self.fcs2_bn(F.relu(self.fcs2(s1)))\n",
    "\t\ta1 = self.fca1_bn(F.relu(self.fca1(action)))\n",
    "\t\t#print(\"\\ns2 in fwd\",s2.size(),\"a1 in fwd\",a1.size())\n",
    "\t\tx = torch.cat((s2,a1),dim=1)\n",
    "\n",
    "\t\tx = self.fc2_bn(F.relu(self.fc2(x)))\n",
    "\t\tx = self.fc3(x)\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "\tdef __init__(self, state_dim, action_dim): #, action_lim\n",
    "\t\t\"\"\"\n",
    "\t\t:param state_dim: Dimension of input state (int)\n",
    "\t\t:param action_dim: Dimension of output action (int)\n",
    "\t\t:param action_lim: Used to limit action in [-action_lim,action_lim]\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(Actor, self).__init__()\n",
    "\n",
    "\t\tself.state_dim = state_dim\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\t#self.action_lim = action_lim\n",
    "\n",
    "\t\tself.fc1 = nn.Linear(state_dim,10)\n",
    "\t\tself.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "\t\tself.fc1_bn = nn.BatchNorm1d(10)\n",
    "\t\tself.fc2 = nn.Linear(10,16)\n",
    "\t\tself.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "\t\tself.fc2_bn = nn.BatchNorm1d(16)\n",
    "\t\tself.fc3 = nn.Linear(16,6)\n",
    "\t\tself.fc3.weight.data = fanin_init(self.fc3.weight.data.size())\n",
    "\t\tself.fc3_bn = nn.BatchNorm1d(6)\n",
    "\t\tself.fc4 = nn.Linear(6,action_dim)\n",
    "\t\tself.fc4.weight.data.uniform_(-EPS,EPS)\n",
    "\n",
    "\tdef forward(self, state):\n",
    "\t\t\"\"\"\n",
    "\t\treturns policy function Pi(s) obtained from actor network\n",
    "\t\tthis function is a gaussian prob distribution for all actions\n",
    "\t\twith mean lying in (-1,1) and sigma lying in (0,1)\n",
    "\t\tThe sampled action can , then later be rescaled\n",
    "\t\t:param state: Input state (Torch Variable : [n,state_dim] )\n",
    "\t\t:return: Output action (Torch Variable: [n,action_dim] )\n",
    "\t\t\"\"\"\n",
    "\t\t#x = state\n",
    "\t\tx = self.fc1_bn(F.relu(self.fc1(state)))\n",
    "\t\tx = self.fc2_bn(F.relu(self.fc2(x)))\n",
    "\t\tx = self.fc3_bn(F.relu(self.fc3(x)))\n",
    "\t\t#x = F.relu(self.fc1(state))        \n",
    "\t\t#x = F.relu(self.fc2(self.fc1_bn(x)))\n",
    "\t\t#x = F.relu(self.fc3(self.fc2_bn(x)))\n",
    "\t\taction = F.relu(self.fc4(x))\n",
    "\n",
    "\t\t#action = action * self.action_lim\n",
    "\n",
    "\t\treturn action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Train ###########\n",
    "\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#import utils\n",
    "#import model\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "\tdef __init__(self, state_dim, action_dim, ram): #, self.action_lim\n",
    "\t\t\"\"\"\n",
    "\t\t:param state_dim: Dimensions of state (int)\n",
    "\t\t:param action_dim: Dimension of action (int)\n",
    "\t\t:param action_lim: Used to limit action in [-action_lim,action_lim]\n",
    "\t\t:param ram: replay memory buffer object\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tself.state_dim = state_dim\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\t#self.action_lim = action_lim\n",
    "\t\tself.ram = ram\n",
    "\t\tself.iter = 0\n",
    "\t\tself.noise = OrnsteinUhlenbeckActionNoise(self.action_dim)\n",
    "\n",
    "\t\tself.actor = Actor(self.state_dim, self.action_dim) # , self.action_lim\n",
    "\t\tself.target_actor = Actor(self.state_dim, self.action_dim) #, self.action_lim\n",
    "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(),LEARNING_RATE)\n",
    "\n",
    "\t\tself.critic = Critic(self.state_dim, self.action_dim)\n",
    "\t\tself.target_critic = Critic(self.state_dim, self.action_dim)\n",
    "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(),LEARNING_RATE)\n",
    "\n",
    "\t\thard_update(self.target_actor, self.actor)\n",
    "\t\thard_update(self.target_critic, self.critic)\n",
    "\t\t\n",
    "        \n",
    "\tdef get_exploitation_action(self, state):\n",
    "\t\t\"\"\"\n",
    "\t\tgets the action from target actor added with exploration noise\n",
    "\t\t:param state: state (Numpy array)\n",
    "\t\t:return: sampled action (Numpy array)\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\taction = self.target_actor.forward(state).detach()\n",
    "\t\treturn action.data.numpy()\n",
    "\n",
    "\tdef get_exploration_action(self, state):\n",
    "\t\t\"\"\"\n",
    "\t\tgets the action from actor added with exploration noise\n",
    "\t\t:param state: state (Numpy array)\n",
    "\t\t:return: sampled action (Numpy array)\n",
    "\t\t\"\"\"\n",
    "\t\t#state = [state_0, state_1, state_2, state_3, state_4]\n",
    "\t\taction = self.actor.forward(state).detach()\n",
    "\t\tnew_action = action.data.numpy() + (self.noise.sample()) # * self.action_lim)\n",
    "\t\treturn new_action\n",
    "\n",
    "\tdef optimize(self):\n",
    "\t\t\"\"\"\n",
    "\t\tSamples a random batch from replay memory and performs optimization\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\t#global loss_critic, loss_actor\n",
    "        \n",
    "\t\ts1,a1,r1,s2 = self.ram.sample(BATCH_SIZE)\n",
    "\t\t#print('\\ns1 in optim',s1,'a1 in optim',a1)\n",
    "\n",
    "\t\ts1 = torch.FloatTensor(s1) #Variable(torch.from_numpy(s1))\n",
    "\t\ta1 = torch.FloatTensor(a1)\n",
    "\t\tr1 = torch.FloatTensor(r1)\n",
    "\t\ts2 = torch.FloatTensor(s2)\n",
    "\t\t#print('\\ns1 in optim after conv',s1.size(),'a1 in optim after conv',a1.size())\n",
    "\n",
    "\t\t# ---------------------- optimize critic ----------------------\n",
    "\t\t# Use target actor exploitation policy here for loss evaluation\n",
    "\t\ta2 = self.target_actor.forward(s2).detach()\n",
    "\t\tnext_val = torch.squeeze(self.target_critic.forward(s2, a2).detach())\n",
    "\t\t# y_exp = r + gamma*Q'( s2, pi'(s2))\n",
    "\t\ty_expected = r1 + GAMMA*next_val\n",
    "\t\t# y_pred = Q( s1, a1)\n",
    "\t\ty_predicted = torch.squeeze(self.critic.forward(s1, a1))\n",
    "\t\t# compute critic loss, and update the critic\n",
    "\t\tloss_critic = F.smooth_l1_loss(y_predicted, y_expected)\n",
    "\n",
    "\n",
    "\n",
    "\t\tself.critic_optimizer.zero_grad()\n",
    "\t\tloss_critic.backward()\n",
    "\t\tself.critic_optimizer.step()\n",
    "\n",
    "\t\t# ---------------------- optimize actor ----------------------\n",
    "\t\tpred_a1 = self.actor.forward(s1)\n",
    "\t\tloss_actor = -1*torch.sum(self.critic.forward(s1, pred_a1))\n",
    "\n",
    "\n",
    "\n",
    "\t\t#print(loss_actor)        \n",
    "\t\tself.actor_optimizer.zero_grad()\n",
    "\t\tloss_actor.backward()\n",
    "\t\tself.actor_optimizer.step()\n",
    "\n",
    "\t\tsoft_update(self.target_actor, self.actor, TAU)\n",
    "\t\tsoft_update(self.target_critic, self.critic, TAU)\n",
    "\n",
    "\t\t# if self.iter % 100 == 0:\n",
    "\t\t# \tprint 'Iteration :- ', self.iter, ' Loss_actor :- ', loss_actor.data.numpy(),\\\n",
    "\t\t# \t\t' Loss_critic :- ', loss_critic.data.numpy()\n",
    "\t\t# self.iter += 1\n",
    "\t#def losses(loss_actor, loss_critic):\n",
    "\t\t#A = loss_actor\n",
    "\t\t#B = loss_critic\n",
    "\t\t#return A, B\n",
    "    #C,D = \n",
    "\tdef save_models(self, episode_count, path_target, path_critic):\n",
    "\t\t\"\"\"\n",
    "\t\tsaves the target actor and critic models\n",
    "\t\t:param episode_count: the count of episodes iterated\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\t        \n",
    "\t\ttorch.save(self.target_actor.state_dict(),  path_target )#, 'C:\\Users\\hp\\Desktop\\Python DDPG') #'./Models/' + str(episode_count) + '_actor.pt'\n",
    "\t\ttorch.save(self.target_critic.state_dict(), path_critic)#, 'C:\\Users\\hp\\Desktop\\Python DDPG') #'./Models/' + str(episode_count) + '_critic.pt'\n",
    "\t\t#print ('Models saved successfully')\n",
    "\t\t#retrn\n",
    "\tdef load_models(self, episode):\n",
    "\t\t\"\"\"\n",
    "\t\tloads the target actor and critic models, and copies them onto actor and critic models\n",
    "\t\t:param episode: the count of episodes iterated (used to find the file name)\n",
    "\t\t:return:\n",
    "\t\t\"\"\"\n",
    "\t\tself.actor.load_state_dict(torch.load('./Models/' + str(episode) + '_actor.pt'))\n",
    "\t\tself.critic.load_state_dict(torch.load('./Models/' + str(episode) + '_critic.pt'))\n",
    "\t\thard_update(self.target_actor, self.actor)\n",
    "\t\thard_update(self.target_critic, self.critic)\n",
    "\t\tprint('Models loaded succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################# Utils #################\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "import torch.autograd as Variable\n",
    "\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "\t\"\"\"\n",
    "\tCopies the parameters from source network (x) to target network (y) using the below update\n",
    "\ty = TAU*x + (1 - TAU)*y\n",
    "\t:param target: Target network (PyTorch)\n",
    "\t:param source: Source network (PyTorch)\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\tfor target_param, param in zip(target.parameters(), source.parameters()):\n",
    "\t\ttarget_param.data.copy_(\n",
    "\t\t\ttarget_param.data * (1.0 - tau) + param.data * tau\n",
    "\t\t)\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "\t\"\"\"\n",
    "\tCopies the parameters from source network to target network\n",
    "\t:param target: Target network (PyTorch)\n",
    "\t:param source: Source network (PyTorch)\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\tfor target_param, param in zip(target.parameters(), source.parameters()):\n",
    "\t\t\ttarget_param.data.copy_(param.data)\n",
    "\n",
    "\n",
    "def save_training_checkpoint(state, is_best, episode_count):\n",
    "\t\"\"\"\n",
    "\tSaves the models, with all training parameters intact\n",
    "\t:param state:\n",
    "\t:param is_best:\n",
    "\t:param filename:\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "\tfilename = str(episode_count) + 'checkpoint.path.rar'\n",
    "\ttorch.save(state, filename)\n",
    "\tif is_best:\n",
    "\t\tshutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "\n",
    "\tdef __init__(self, action_dim, mu = 0, theta = 0.15, sigma = 0.2):\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.mu = mu\n",
    "\t\tself.theta = theta\n",
    "\t\tself.sigma = sigma\n",
    "\t\tself.X = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.X = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "\tdef sample(self):\n",
    "\t\tdx = self.theta * (self.mu - self.X)\n",
    "\t\tdx = dx + self.sigma * np.random.randn(len(self.X))\n",
    "\t\tself.X = self.X + dx\n",
    "\t\treturn self.X\n",
    "\n",
    "\n",
    "# use this to plot Ornstein Uhlenbeck random motion\n",
    "if __name__ == '__main__':\n",
    "\tou = OrnsteinUhlenbeckActionNoise(1)\n",
    "\tstates = []\n",
    "\tfor i in range(1000):\n",
    "\t\tstates.append(ou.sample())\n",
    "\timport matplotlib.pyplot as plt\n",
    "\n",
    "\tplt.plot(states)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment - Bulk Good System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Declaration before \n",
    "Silo_Filllevel_1 = 11.6133\n",
    "Silo_Filllevel_2 = 11.6133\n",
    "Silo_Filllevel_3 = 11.6133\n",
    "\n",
    "Hopper_Filllevel_1 = 4.55\n",
    "Hopper_Filllevel_2 = 0\n",
    "Hopper_Filllevel_3 = 0\n",
    "\n",
    "if Silo_Filllevel_1 >= 1:\n",
    "    Silo_Empty_1 = 1\n",
    "else:\n",
    "    Silo_Empty_1 = 0\n",
    "            \n",
    "if Silo_Filllevel_2 >= 1:\n",
    "    Silo_Empty_2 = 1\n",
    "else:\n",
    "    Silo_Empty_2 = 0\n",
    "          \n",
    "if Silo_Filllevel_3 >= 1:\n",
    "    Silo_Empty_3 = 1\n",
    "else:\n",
    "    Silo_Empty_3 = 0\n",
    "    \n",
    "if Hopper_Filllevel_1 >= 1:\n",
    "    Hopper_Empty_1 = 1\n",
    "else:\n",
    "    Hopper_Empty_1 = 0\n",
    "            \n",
    "if Hopper_Filllevel_2 >= 1:\n",
    "    Hopper_Empty_2 = 1\n",
    "else:\n",
    "    Hopper_Empty_2 = 0\n",
    "            \n",
    "if Hopper_Filllevel_3 >= 1:\n",
    "    Hopper_Empty_3 = 1\n",
    "else:\n",
    "    Hopper_Empty_3 = 0\n",
    "\n",
    "VP_Prev_2 = 0\n",
    "VP_Prev_3 = 0\n",
    "\n",
    "Rising_2 = 0\n",
    "Falling_2 = 0\n",
    "Rising_3 = 0\n",
    "Falling_3 = 0\n",
    "\n",
    "Q_Flip_2 = 0\n",
    "Q_Flip_3 = 0\n",
    "\n",
    "Q_Flip_Prev_2 = 0\n",
    "Q_Flip_Prev_3 = 0\n",
    "\n",
    "Saturator_2 = 0\n",
    "Saturator_3 = 0\n",
    "\n",
    "MF_S_H_1 = 0\n",
    "MF_S_H_2 = 0\n",
    "MF_S_H_3 = 0\n",
    "\n",
    "MF_VC_2 = 0\n",
    "MF_VC_3 = 0\n",
    "\n",
    "Energy_Conveyor_Sim = 0\n",
    "Energy_VC_2_Sim = 0\n",
    "Energy_Vibration_Belt_Sim = 0\n",
    "Energy_VC_3_Sim = 0\n",
    "Energy_Rotary_Air_Lock_Sim = 0\n",
    "\n",
    "Silo_Fill_1_Error = []\n",
    "Time = []\n",
    "###########\n",
    "Conveyor_Motor_Speed = 0\n",
    "Vibration_Belt_Start = 1\n",
    "Dosing_Speed = 600\n",
    "Demand = 50\n",
    "\n",
    "VP_2 = 0\n",
    "VP_3 = 1\n",
    "\n",
    "VP_Time_2 = 10\n",
    "VP_Time_3 = 10\n",
    "\n",
    "Exchanged_Mass_Sim = 0\n",
    "reward_1 = 0\n",
    "reward_2 = 0\n",
    "reward_3 = 0\n",
    "reward_4 = 0\n",
    "reward_5 = 0\n",
    "\n",
    "Total_Reward_1 = 0\n",
    "Total_Reward_2 = 0\n",
    "Total_Reward_3 = 0\n",
    "Total_Reward_4 = 0\n",
    "Total_Reward_5 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Initial State for an episode ###################\n",
    "import random\n",
    "from random import randrange, uniform\n",
    "\n",
    "#if action_counter == 1:\n",
    "def initial_state():\n",
    "    \n",
    "    global Silo_Filllevel_1, Silo_Filllevel_2, Silo_Filllevel_3, Hopper_Filllevel_1, Hopper_Filllevel_2, Hopper_Filllevel_3\n",
    "    global Silo_Empty_1, Hopper_Empty_1, Q_Flip_2, Q_Flip_3, Q_Flip_Prev_2, Q_Flip_Prev_3, Saturator_2, Saturator_3\n",
    "    global VP_Prev_2, VP_Prev_3\n",
    "    \n",
    "    Silo_Filllevel_1 = uniform(0,11.42)\n",
    "    Silo_Filllevel_2 = uniform(0,11.42)\n",
    "    Silo_Filllevel_3 = uniform(0,11.42)\n",
    "    \n",
    "    Hopper_Filllevel_1 = uniform(0,9.1)\n",
    "    Hopper_Filllevel_2 = uniform(0,9.1)\n",
    "    Hopper_Filllevel_3 = uniform(0,9.1)\n",
    "    \n",
    "    if Silo_Filllevel_1 >= 1:\n",
    "        Silo_Empty_1 = 1\n",
    "    else:\n",
    "        Silo_Empty_1 = 0\n",
    "            \n",
    "    if Silo_Filllevel_2 >= 1:\n",
    "        Silo_Empty_2 = 1\n",
    "    else:\n",
    "        Silo_Empty_2 = 0\n",
    "          \n",
    "    if Silo_Filllevel_3 >= 1:\n",
    "        Silo_Empty_3 = 1\n",
    "    else:\n",
    "        Silo_Empty_3 = 0\n",
    "    \n",
    "    if Hopper_Filllevel_1 >= 1:\n",
    "        Hopper_Empty_1 = 1\n",
    "    else:\n",
    "        Hopper_Empty_1 = 0\n",
    "            \n",
    "    if Hopper_Filllevel_2 >= 1:\n",
    "        Hopper_Empty_2 = 1\n",
    "    else:\n",
    "        Hopper_Empty_2 = 0\n",
    "            \n",
    "    if Hopper_Filllevel_3 >= 1:\n",
    "        Hopper_Empty_3 = 1\n",
    "    else:\n",
    "        Hopper_Empty_3 = 0\n",
    "    \n",
    "    VP_Prev_2 = randrange(0,2)\n",
    "    VP_Prev_3 = randrange(0,2)\n",
    "\n",
    "    Rising_2 = randrange(0,2)\n",
    "    Falling_2 = randrange(0,2)\n",
    "    Rising_3 = randrange(0,2)\n",
    "    Falling_3 = randrange(0,2)\n",
    "\n",
    "    Q_Flip_2 = randrange(0,2)\n",
    "    Q_Flip_3 = randrange(0,2)\n",
    "\n",
    "    Q_Flip_Prev_2 = randrange(0,2)\n",
    "    Q_Flip_Prev_3 = randrange(0,2)\n",
    "\n",
    "    Saturator_2 = randrange(0,2)\n",
    "    Saturator_3 = randrange(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### State Space Reset ################\n",
    "\n",
    "def state_space_reset():\n",
    "    global Energy_Conveyor_Sim, Energy_Vibration_Belt_Sim, Energy_VC_2_Sim, Energy_VC_3_Sim, Energy_Rotary_Air_Lock_Sim\n",
    "    global reward_1, reward_2, reward_3, Total_Reward_1, Total_Reward_2, Total_Reward_3, Demand, Total_Energy_Sim, Exchanged_Mass_Sim, reached\n",
    "    global sum_of_actor_loss_1, sum_of_critic_loss_1,sum_of_actor_loss_2, sum_of_critic_loss_2,sum_of_actor_loss_3, sum_of_critic_loss_3,sum_of_actor_loss_4, sum_of_critic_loss_4,sum_of_actor_loss_5, sum_of_critic_loss_5\n",
    "    Exchanged_Mass_Sim = 0\n",
    "    Energy_Conveyor_Sim = 0                \n",
    "    Energy_VC_2_Sim = 0\n",
    "    Energy_Vibration_Belt_Sim = 0\n",
    "    Energy_VC_3_Sim = 0\n",
    "    Energy_Rotary_Air_Lock_Sim = 0\n",
    "    Total_Energy_Sim = 0\n",
    "    reached = 0   \n",
    "    Total_Reward_1 = 0\n",
    "    Total_Reward_2 = 0\n",
    "    Total_Reward_3 = 0\n",
    "    reward_1 = 0\n",
    "    reward_2 = 0\n",
    "    reward_3 = 0\n",
    "    Demand = 0\n",
    "    Energy_Episode = 0\n",
    "    sum_of_actor_loss_1 = 0\n",
    "    sum_of_critic_loss_1 = 0\n",
    "    sum_of_actor_loss_2 = 0\n",
    "    sum_of_critic_loss_2 = 0\n",
    "    sum_of_actor_loss_3 = 0\n",
    "    sum_of_critic_loss_3 = 0\n",
    "    sum_of_actor_loss_4 = 0\n",
    "    sum_of_critic_loss_4 = 0\n",
    "    sum_of_actor_loss_5 = 0\n",
    "    sum_of_critic_loss_5 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# ENVIRONMENT ##############################################\n",
    "########################################################################################################\n",
    "\n",
    "##### The below lines of code are for the NN to realize the working ranges of various variables #####\n",
    "def learning_action_range():\n",
    "    \n",
    "    global Conveyor_Motor_Speed, Vibration_Belt_Start, VP_2, VP_Time_2, VP_3, VP_Time_3, Dosing_Speed, Demand\n",
    "    global reward_1, reward_2, reward_3\n",
    "    \n",
    "    if Conveyor_Motor_Speed >  1850 or Conveyor_Motor_Speed < 450:\n",
    "        reward_1 = -30  \n",
    "        reward_2 = -30 \n",
    "        reward_3 = -30\n",
    "    elif Vibration_Belt_Start != 0 or Vibration_Belt_Start != 1:\n",
    "        reward_1 = -30  \n",
    "        reward_2 = -30  \n",
    "        reward_3 = -30\n",
    "    elif VP_Time_2 > 10 or VP_Time_2 < 0:\n",
    "        reward_1 = -30  \n",
    "        reward_2 = -30  \n",
    "        reward_3 = -30\n",
    "    elif VP_Time_3 > 10 or VP_Time_3 < 0:\n",
    "        reward_1 = -30 \n",
    "        reward_2 = -30 \n",
    "        reward_3 = -30\n",
    "    elif Dosing_Speed > 1500 or Dosing_Speed < 450:\n",
    "        reward_1 = -30  \n",
    "        reward_2 = -30 \n",
    "        reward_3 = -30\n",
    "    elif Demand > 100:\n",
    "        reward_1 = -30 \n",
    "        reward_2 = -30 \n",
    "        reward_3 = -30\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### BULK GOOD SYSTEM #####################################\n",
    "import scipy.integrate as integrate\n",
    "from scipy.integrate import quad\n",
    "from math import pow\n",
    "\n",
    "def environment_bgs():\n",
    "    \n",
    "    global Silo_Filllevel_1, Silo_Filllevel_2, Silo_Filllevel_3, Hopper_Filllevel_1, Hopper_Filllevel_2, Hopper_Filllevel_3\n",
    "    global MF_VC_2, MF_VC_3,Demand\n",
    "    global Energy_Conveyor_Sim, Energy_VC_2_Sim, Energy_Vibration_Belt_Sim, Energy_VC_3_Sim, Energy_Rotary_Air_Lock_Sim\n",
    "    global VP_2, VP_3, VP_Time_2, VP_Time_3, MF_S_H_1, MF_S_H_2, MF_S_H_3\n",
    "    global Silo_Empty_1, Hopper_Empty_1, Q_Flip_2, Q_Flip_3, Q_Flip_Prev_2, Q_Flip_Prev_3, Saturator_2, Saturator_3\n",
    "    global VP_Prev_2, VP_Prev_3, Exchanged_Mass_Sim, reward_1, reward_2, reward_3, reached\n",
    "    global Total_Reward_1, Total_Reward_2, Total_Reward_3, Demand\n",
    "    \n",
    "    bigloop = 0\n",
    "    t = 0\n",
    "    while bigloop <=100:\n",
    "\n",
    "        ##################### Station 1 #####################\n",
    "\n",
    "        #### Substation 1 - Conveyor Belt - RPM to Mass flow ####\n",
    "\n",
    "\n",
    "        # Massflow Silo to Hopper\n",
    "\n",
    "        # Silo Empty\n",
    "        #Silo_Filllevel_1 = uniform(0,11.42)\n",
    "        \n",
    "        if Silo_Filllevel_1 >= 1:\n",
    "            Silo_Empty_1 = 1\n",
    "        else:\n",
    "            Silo_Empty_1 = 0\n",
    "\n",
    "        MF_S_H_1 = Conveyor_Motor_Speed * 0.01 / 60 * Silo_Empty_1\n",
    "\n",
    "        cms = Conveyor_Motor_Speed\n",
    "\n",
    "        # Energy - Conveyor Belt\n",
    "\n",
    "        f_cms = lambda x: Conveyor_Motor_Speed*(-81.0493 + 0.5558316*cms - 0.0008993839*cms**2 + 6.578498e-7*cms**3 - 2.197611e-10*cms**4 + 2.721772e-14*cms**5)/(60*1000)\n",
    "        energy,err = integrate.quad(f_cms,t,t+0.1)\n",
    "\n",
    "        Energy_Conveyor_Sim = Energy_Conveyor_Sim +  energy\n",
    "\n",
    "        #### Substation 2 - Silo ####\n",
    "\n",
    "        # Silo Fill level\n",
    "\n",
    "        f_sfl_1 = lambda x: MF_S_H_1 * (-1)\n",
    "        fill,err = integrate.quad(f_sfl_1,t,t+0.1)\n",
    "\n",
    "        #torch.save(Silo_Fill_1_Error, 'Silo_Fill_1_Error.pt')\n",
    "\n",
    "        Silo_Filllevel_1 = Silo_Filllevel_1 + fill\n",
    "        #Mass_Silo_1 = Mass_Silo_1 + fill\n",
    "        \n",
    "        #Silo_Fill_1_Error.append(Silo_Filllevel_1)\n",
    "\n",
    "        if Silo_Filllevel_1 >= 17.42:\n",
    "            Silo_Filllevel_1 = 17.42\n",
    "        elif Silo_Filllevel_1 <= 0:\n",
    "            Silo_Filllevel_1 = 0\n",
    "\n",
    "        # Silo Empty\n",
    "\n",
    "        if Silo_Filllevel_1 >= 1:\n",
    "            Silo_Empty_1 = 1\n",
    "        else:\n",
    "            Silo_Empty_1 = 0\n",
    "\n",
    "        #### Substation 3 - Hopper ####\n",
    "\n",
    "        # Hopper Fill level\n",
    "        f_hfl_1 = lambda x: (MF_S_H_1 - MF_VC_2)\n",
    "\n",
    "        fill, err = integrate.quad(f_hfl_1, t, t+0.1)\n",
    "        Hopper_Filllevel_1 = Hopper_Filllevel_1 + fill\n",
    "\n",
    "        if Hopper_Filllevel_1 >= 17.42:\n",
    "            Hopper_Filllevel_1 = 17.42\n",
    "        elif Hopper_Filllevel_1 <= 0:\n",
    "            Hopper_Filllevel_1 = 0\n",
    "\n",
    "        # Hopper Empty\n",
    "\n",
    "        if Hopper_Filllevel_1 >= 1:\n",
    "            Hopper_Empty_1 = 1\n",
    "        else:\n",
    "            Hopper_Empty_1 = 0\n",
    "\n",
    "        ##################### Station 2 ##################### \n",
    "\n",
    "        #### Substation 1 - Vacuum Pump ####\n",
    "\n",
    "        # Massflow - Previous Hopper to Silo\n",
    "\n",
    "        if t >= VP_Time_2:\n",
    "            VP_2 = 0\n",
    "        else:\n",
    "            VP_2 = 1\n",
    "        \n",
    "        if VP_2 - VP_Prev_2 > 0:              # To know if the quantity of material is rising or falling in the VP\n",
    "            Rising_2 = 1\n",
    "            Falling_2 = 0     \n",
    "        elif VP_2 - VP_Prev_2 <0:             #\n",
    "            Rising_2 = 0 \n",
    "            Falling_2 = 1     #\n",
    "        else:                                  #\n",
    "            Rising_2 = 0 \n",
    "            Falling_2 = 0     #\n",
    "\n",
    "        VP_Prev_2 = VP_2\n",
    "\n",
    "        if Rising_2 == 0 and Falling_2 == 0:    # SR Flip-Flop equivalent\n",
    "            Q_Flip_2 = Q_Flip_Prev_2\n",
    "        elif Rising_2 == 0 and Falling_2 == 1:\n",
    "            Q_Flip_2 = 1\n",
    "        elif Rising_2 == 1 and Falling_2 == 0:\n",
    "            Q_Flip_2 = 0\n",
    "        elif Rising_2 == 1 and Falling_2 == 1:\n",
    "            Q_Flip_2 = 0\n",
    "\n",
    "        Q_Flip_Prev_2 = Q_Flip_2\n",
    "\n",
    "        if Q_Flip_2 == 0:\n",
    "            MF_VC_2 = 0\n",
    "        elif Q_Flip_2 == 0:\n",
    "            Saturator_2 = t - 0.567\n",
    "\n",
    "        if Saturator_2 >= 4.575:\n",
    "            Saturator_2 = 4.575\n",
    "        elif Saturator_2 <= 0:\n",
    "            Saturator_2 = 0\n",
    "\n",
    "\n",
    "        MF_VC_2 = ((0.0664 * Saturator_2) + 0.464) * (Hopper_Empty_1)\n",
    "\n",
    "        # Energy - Vacuum Pump\n",
    "\n",
    "        f_vp_2 = lambda x: (VP_2 * 0.305)\n",
    "\n",
    "        energy,err = integrate.quad(f_vp_2, t, t+0.1)\n",
    "\n",
    "        Energy_VC_2_Sim = Energy_VC_2_Sim + energy\n",
    "\n",
    "        #### Substation 2- Vibration Belt ####\n",
    "\n",
    "        # Massflow Silo to Hopper\n",
    "\n",
    "        # Silo Empty\n",
    "\n",
    "        if Silo_Filllevel_2 >= 1:\n",
    "            Silo_Empty_2 = 1\n",
    "        else:\n",
    "            Silo_Empty_2 = 0\n",
    "\n",
    "        MF_S_H_2 = Vibration_Belt_Start * 0.4 * Silo_Empty_2\n",
    "\n",
    "        # Energy Vibration Belt\n",
    "\n",
    "        f_env = lambda x: (26.9 * Vibration_Belt_Start / 1000)\n",
    "        energy, err = integrate.quad(f_env,t,t+0.1)\n",
    "\n",
    "        Energy_Vibration_Belt_Sim = Energy_Vibration_Belt_Sim + energy\n",
    "\n",
    "        #### Substation 3 - Silo ####\n",
    "\n",
    "        # Silo Fill level\n",
    "\n",
    "        f_sfl_2 = lambda x: (MF_VC_2 - MF_S_H_2)\n",
    "        fill,err = integrate.quad(f_sfl_2,t,t+0.1)\n",
    "\n",
    "        Silo_Filllevel_2  = Silo_Filllevel_2 + fill\n",
    "        #Mass_Silo_2 = Mass_Silo_2 + fill\n",
    "\n",
    "        if Silo_Filllevel_2 >= 17.42:\n",
    "            Silo_Filllevel_2 = 17.42\n",
    "        elif Silo_Filllevel_2 <= 0:\n",
    "            Silo_Filllevel_2 = 0\n",
    "\n",
    "\n",
    "        # Silo Empty\n",
    "\n",
    "        if Silo_Filllevel_2 >= 1:\n",
    "            Silo_Empty_2 = 1\n",
    "        else:\n",
    "            Silo_Empty_2 = 0\n",
    "\n",
    "        #### Substation 3 - Hopper ####\n",
    "\n",
    "        # Hopper Fill level\n",
    "\n",
    "        f_hfl_2 = lambda x: (MF_S_H_2 - MF_VC_3)\n",
    "\n",
    "        fill,err = integrate.quad(f_hfl_2,t,t+0.1)\n",
    "\n",
    "        Hopper_Filllevel_2 = Hopper_Filllevel_2 + fill\n",
    "\n",
    "        if Hopper_Filllevel_2 >= 17.42:\n",
    "            Hopper_Filllevel_2 = 17.42\n",
    "        elif Hopper_Filllevel_2 <= 0:\n",
    "            Hopper_Filllevel_2 = 0\n",
    "\n",
    "        # Hopper Empty\n",
    "\n",
    "        if Hopper_Filllevel_2 >= 1:\n",
    "            Hopper_Empty_2 = 1\n",
    "        else:\n",
    "            Hopper_Empty_2 = 0\n",
    "\n",
    "\n",
    "        ##################### Station 3 ##################### \n",
    "\n",
    "        #### Substation 1 - Vacuum Pump ####\n",
    "\n",
    "        if t >= VP_Time_3:\n",
    "            VP_3 = 0\n",
    "        else: \n",
    "            VP_3 = 0\n",
    "\n",
    "        if VP_3 - VP_Prev_3 > 0:              # To know if the quantity of material is rising or falling in the VP\n",
    "            Rising_3 = 1 \n",
    "            Falling_3 = 0      #\n",
    "        elif VP_3 - VP_Prev_2 <0:             #\n",
    "            Rising_3 = 0 \n",
    "            Falling_3 = 1      #\n",
    "        elif VP_3 - VP_Prev_3 == 0:            #\n",
    "            Rising_3 = 0 \n",
    "            Falling_3 = 0      #\n",
    "\n",
    "        VP_Prev_3 = VP_3\n",
    "\n",
    "        if Rising_3 == 0 and Falling_3 == 0:    # SR Flip-Flop equivalent\n",
    "            Q_Flip_3 = Q_Flip_Prev_3\n",
    "        elif Rising_3 == 0 and Falling_3 == 1:\n",
    "            Q_Flip_3 = 1\n",
    "        elif Rising_3 == 1 and Falling_3 == 0:\n",
    "            Q_Flip_3 = 0\n",
    "        elif Rising_3 == 1 and Falling_3 == 1:\n",
    "            Q_Flip_3 = 0\n",
    "\n",
    "        Q_Flip_Prev_3 = Q_Flip_3\n",
    "\n",
    "        if Q_Flip_3 == 0:\n",
    "            MF_VC_3 = 0\n",
    "        elif Q_Flip_3 == 0:\n",
    "            Saturator_3 = t - 0.979\n",
    "\n",
    "        if Saturator_3 >= 9.5:\n",
    "            Saturator_3 = 9.5\n",
    "        elif Saturator_3 <= 0:\n",
    "            Saturator_3 = 0\n",
    "\n",
    "\n",
    "\n",
    "        MF_VC_3 = ((0.0192 * Saturator_3) + 0.3535) * (Hopper_Empty_2)\n",
    "\n",
    "        # Energy - Vacuum Pump\n",
    "\n",
    "        f_vp_3 = lambda x: (VP_3 * 0.456)\n",
    "\n",
    "        energy,err = integrate.quad(f_vp_3,t,t+0.1)\n",
    "\n",
    "        Energy_VC_3_Sim = Energy_VC_3_Sim + energy\n",
    "\n",
    "        #### Substation 2 - Rotary Air lock ####\n",
    "\n",
    "        # Massflow Silo to Hopper\n",
    "\n",
    "        # Silo Empty\n",
    "\n",
    "        if Silo_Filllevel_3 >= 1:\n",
    "            Silo_Empty_3 = 1\n",
    "        else:\n",
    "            Silo_Empty_3 = 0\n",
    "\n",
    "        MF_S_H_3 = (Dosing_Speed) * (Silo_Empty_3) * 0.01249 / 60\n",
    "\n",
    "        # Energy Rotary Air Lock\n",
    "\n",
    "        f_eral = lambda x: (Dosing_Speed * 370 / (1500 * 1000))\n",
    "\n",
    "        energy,err = integrate.quad(f_eral,t,t+0.1)\n",
    "\n",
    "        Energy_Rotary_Air_Lock_Sim = Energy_Rotary_Air_Lock_Sim + energy\n",
    "\n",
    "        #### Substation 3 - Silo ####\n",
    "\n",
    "        # Silo Fill level\n",
    "\n",
    "        f_sfl_3 = lambda x: (MF_VC_3 - MF_S_H_3)\n",
    "        fill,err = integrate.quad(f_sfl_3,t,t+0.1)\n",
    "\n",
    "        Silo_Filllevel_3  = Silo_Filllevel_3 + fill\n",
    "        #Mass_Silo_3 = Mass_Silo_3 + fill\n",
    "\n",
    "        if Silo_Filllevel_3 >= 17.42:\n",
    "            Silo_Filllevel_3 = 17.42\n",
    "        elif Silo_Filllevel_3 <= 0:\n",
    "            Silo_Filllevel_3 = 0\n",
    "\n",
    "        # Silo Empty\n",
    "\n",
    "        if Silo_Filllevel_3 >= 1:\n",
    "            Silo_Empty_3 = 1\n",
    "        else:\n",
    "            Silo_Empty_3 = 0\n",
    "\n",
    "        #### Substation 3 - Hopper ####\n",
    "\n",
    "        # Hopper Fill level\n",
    "\n",
    "        f_hfl_3 = lambda x: (MF_S_H_1 - Demand)\n",
    "\n",
    "        fill,err = integrate.quad(f_hfl_3,t,t+0.1)\n",
    "        \n",
    "        Hopper_Filllevel_3 = Hopper_Filllevel_3 + fill\n",
    "\n",
    "        if Hopper_Filllevel_3 >= 17.42:\n",
    "            Hopper_Filllevel_3 = 17.42\n",
    "        elif Hopper_Filllevel_3 <= 0:\n",
    "            Hopper_Filllevel_3 = 0\n",
    "    \n",
    "        f_demand = lambda x: Demand                      # Overall Mass transported by the system\n",
    "        dem,err = integrate.quad(f_demand,t,t+0.1)       #\n",
    "        Exchanged_Mass_Sim = Exchanged_Mass_Sim + dem    #\n",
    "\n",
    "        # Hopper Empty\n",
    "\n",
    "        if Hopper_Filllevel_3 >= 1:\n",
    "            Hopper_Empty_3 = 1\n",
    "        else:\n",
    "            Hopper_Empty_3 = 0\n",
    "        ##########################################################\n",
    "\n",
    "        t = t + 0.1                             # Next time step \n",
    "\n",
    "        bigloop = bigloop + 1\n",
    "        ################################################################################################################   \n",
    "    Total_Energy_Sim = sum([Energy_Conveyor_Sim, Energy_VC_2_Sim, Energy_Vibration_Belt_Sim, Energy_VC_3_Sim, Energy_Rotary_Air_Lock_Sim])\n",
    "\n",
    "    ############ Rewards ###########    \n",
    "\n",
    "    #### Terminal State ####\n",
    "    if Exchanged_Mass_Sim >= 12000:\n",
    "        reward_1 = 5000 - Energy_Conveyor_Sim\n",
    "        reward_2 = 5000 - Energy_VC_2_Sim\n",
    "        reward_3 = 5000 - Energy_Vibration_Belt_Sim\n",
    "        reward_4 = 5000 - Energy_VC_3_Sim\n",
    "        reward_5 = 5000 - Energy_Rotary_Air_Lock_Sim\n",
    "        reached = 1\n",
    "\n",
    "    else:\n",
    "        reward_1 = -1\n",
    "        reward_2 = -1\n",
    "        reward_3 = -1\n",
    "        reward_4 = -1\n",
    "        reward_5 = -1\n",
    "        reached = 0\n",
    "        \n",
    "    \n",
    "    Total_Reward_1 = Total_Reward_1 + reward_1\n",
    "    Total_Reward_2 = Total_Reward_2 + reward_2\n",
    "    Total_Reward_3 = Total_Reward_3 + reward_3\n",
    "    Total_Reward_4 = Total_Reward_4 + reward_4\n",
    "    Total_Reward_5 = Total_Reward_5 + reward_5\n",
    "    \n",
    "    return Silo_Filllevel_1, Silo_Filllevel_2, Silo_Filllevel_3, Hopper_Filllevel_1, Hopper_Filllevel_2, Hopper_Filllevel_3, Energy_Conveyor_Sim, Energy_VC_2_Sim, Energy_Vibration_Belt_Sim, Energy_VC_3_Sim, Energy_Rotary_Air_Lock_Sim, Total_Energy_Sim, VP_2, VP_3, VP_Time_2, VP_Time_3, MF_VC_2, MF_VC_3, MF_S_H_1, MF_S_H_2, MF_S_H_3, Silo_Empty_1, Hopper_Empty_1, Q_Flip_2, Q_Flip_3, Q_Flip_Prev_2, Q_Flip_Prev_3, Saturator_2, Saturator_3, VP_Prev_2, VP_Prev_3, Exchanged_Mass_Sim, reached, reward_1, reward_2. reward_3, Total_Reward_1, Total_Reward_2, Total_Reward_3, Demand, Total_Energy_Sim \n",
    "\n",
    "# 0 Silo_Filllevel_1      # 7 Energy_VC_2_Sim              #14 VP_Time_2    #21 Silo_Empty_1      #28 Saturator_3          #35 reward_3\n",
    "# 1 Silo_Filllevel_2      # 8 Energy_Vibration_Belt_Sim    #15 VP_Time_3    #22 Hopper_Empty_1    #29 VP_Prev_2            #36 Total_Reward_1\n",
    "# 2 Silo_Filllevel_3      # 9 Energy_VC_3_Sim              #16 MF_VC_2      #23 Q_Flip_2          #30 VP_Prev_3            #37 Total_Reward_2\n",
    "# 3 Hopper_Filllevel_1    #10 Energy_Rotary_Air_Lock_Sim   #17 MF_VC_3      #24 Q_Flip_3          #31 Exchanged_Mass_Sim   #38 Total_Reward_1\n",
    "# 4 Hopper_Filllevel_2    #11 Total_Energy_Sim             #18 MF_S_H_1     #25 Q_Flip_Prev_2     #32 reached              #39 Demand\n",
    "# 5 Hopper_Filllevel_3    #12 VP_2                         #19 MF_S_H_2     #26 Q_Flip_Prev_3     #33 reward_1             #40 Total_Energy_Sim\n",
    "# 6 Energy_Conveyor_Sim   #13 VP_3                         #20 MF_S_H_3     #27 Saturator_2       #34 reward_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 2D or 3D input (got 1D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-927bcde2d46c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mrandomizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandomizer\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mConveyor_Motor_Speed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exploration_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mVP__Time_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exploration_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mVibration_Belt_Start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exploration_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_space_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c41a29797658>\u001b[0m in \u001b[0;36mget_exploration_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     65\u001b[0m \t\t\"\"\"\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m#state = [state_0, state_1, state_2, state_3, state_4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mnew_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# * self.action_lim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnew_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7ee6876ce888>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    102\u001b[0m \t\t\"\"\"\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m#x = state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Python3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Python3.7/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_input_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# exponential_average_factor is self.momentum set to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Python3.7/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m_check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             raise ValueError('expected 2D or 3D input (got {}D input)'\n\u001b[0;32m--> 176\u001b[0;31m                              .format(input.dim()))\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected 2D or 3D input (got 1D input)"
     ]
    }
   ],
   "source": [
    "############ Main #############\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "from random import randrange, uniform\n",
    "\n",
    "#import train\n",
    "#import buffer\n",
    "\n",
    "MAX_EPISODES = 1\n",
    "MAX_STEPS = 6\n",
    "MAX_BUFFER = 100000\n",
    "\n",
    "Energy_Consumed_Per_Episode = []\n",
    "Reward_Per_Episode_1 = []\n",
    "Reward_Per_Episode_2 = []\n",
    "Reward_Per_Episode_3 = []\n",
    "Reward_Per_Episode_4 = []\n",
    "Reward_Per_Episode_5 = []\n",
    "Actor_loss_per_episode_1 = []\n",
    "Critic_loss_per_episode_1 = []\n",
    "Actor_loss_per_episode_2 = []\n",
    "Critic_loss_per_episode_2 = []\n",
    "Actor_loss_per_episode_3 = []\n",
    "Critic_loss_per_episode_3 = []\n",
    "Actor_loss_per_episode_4 = []\n",
    "Critic_loss_per_episode_4 = []\n",
    "Actor_loss_per_episode_5 = []\n",
    "Critic_loss_per_episode_5 = []\n",
    "\n",
    "\n",
    "#MAX_TOTAL_REWARD = 300\n",
    "#S_DIM = env.observation_space.shape[0]\n",
    "#A_DIM = env.action_space.shape[0]\n",
    "#A_MAX = env.action_space.high[0]\n",
    "\n",
    "#print (' State Dimensions :- ', S_DIM)\n",
    "#print (' Action Dimensions :- ', A_DIM)\n",
    "#print (' Action Max :- ', A_MAX)\n",
    "\n",
    "ram_1 = MemoryBuffer(MAX_BUFFER)\n",
    "ram_2 = MemoryBuffer(MAX_BUFFER)\n",
    "ram_3 = MemoryBuffer(MAX_BUFFER)\n",
    "ram_4 = MemoryBuffer(MAX_BUFFER)\n",
    "ram_5 = MemoryBuffer(MAX_BUFFER)\n",
    "\n",
    "trainer_1 = Trainer(5, 1, ram_1)\n",
    "trainer_2 = Trainer(5, 1, ram_2) # , A_MAX\n",
    "trainer_3 = Trainer(5, 1, ram_3) # , A_MAX\n",
    "trainer_4 = Trainer(5, 1, ram_4) # , A_MAX\n",
    "trainer_5 = Trainer(5, 1, ram_5) # , A_MAX\n",
    "\n",
    "for _ep in range(MAX_EPISODES):\n",
    "    state_space_reset()\n",
    "    initial_state()\n",
    "    Demand = uniform(10,50)\n",
    "    \n",
    "    for r in range(200):\n",
    "        #env.render()\n",
    "        \n",
    "        state_1 = [Silo_Filllevel_1, Hopper_Filllevel_1, MF_VC_2, Demand, Demand]\n",
    "        state_2 = [Silo_Filllevel_2, Hopper_Filllevel_2, MF_VC_2, MF_VC_3, Demand]\n",
    "        state_3 = [Silo_Filllevel_3, Hopper_Filllevel_3, MF_VC_2, MF_VC_3, Demand]\n",
    "        state_space_1 = torch.tensor(state_1)\n",
    "        state_space_2 = torch.tensor(state_2)\n",
    "        state_space_3 = torch.tensor(state_3)\n",
    "        \n",
    "        prob = (-0.04)*_ep + 90\n",
    "        randomizer = randrange(0, 101)\n",
    "        if randomizer < prob:\n",
    "            Conveyor_Motor_Speed = trainer_1.get_exploration_action(state_space_1)\n",
    "            VP__Time_2 = trainer_2.get_exploration_action(state_space_2)\n",
    "            Vibration_Belt_Start = trainer_3.get_exploration_action(state_space_2)\n",
    "            VP_Time_3 = trainer_4.get_exploration_action(state_space_3)\n",
    "            Dosing_Speed = trainer_5.get_exploration_action(state_space_3)\n",
    "            #print('Explore')\n",
    "            \n",
    "        else:\n",
    "            Conveyor_Motor_Speed = trainer_1.get_exploitation_action(state_space_1)\n",
    "            VP__Time_2 = trainer_2.get_exploitation_action(state_space_2)\n",
    "            Vibration_Belt_Start = trainer_3.get_exploitation_action(state_space_2)\n",
    "            VP_Time_3 = trainer_4.get_exploitation_action(state_space_3)\n",
    "            Dosing_Speed = trainer_5.get_exploitation_action(state_space_3)\n",
    "            #print('Exploit')\n",
    "            #print(Conveyor_Motor_Speed, VP_Time_2, VP_Time_3, Dosing_Speed, Vibration_Belt_Start)\n",
    "            \n",
    "        #action_1 = trainer_1.get_exploration_action(state)\n",
    "        # if _ep%5 == 0:\n",
    "        # \t# validate every 5th episode\n",
    "        # \taction = trainer.get_exploitation_action(state)\n",
    "        # else:\n",
    "        # \t# get action based on observation, use exploration policy here\n",
    "        # \taction = trainer.get_exploration_action(state)\n",
    "        #new_observation, reward, done, info = env.step(action)\n",
    "\n",
    "        working_range = 0\n",
    "        \n",
    "        if Conveyor_Motor_Speed >  1850 or Conveyor_Motor_Speed < 450:\n",
    "            reward_1 = -30  \n",
    "            working_range = 1\n",
    "        if VP_Time_2 != 0 or VP_Time_2 != 1: \n",
    "            reward_2 = -30  \n",
    "            working_range = 1\n",
    "        if Vibration_Belt_Start > 10 or Vibration_Belt_Start < 0: \n",
    "            reward_3 = -30  \n",
    "            working_range = 1\n",
    "        if VP_Time_3 > 10 or VP_Time_3 < 0:\n",
    "            reward_4 = -30\n",
    "            working_range = 1\n",
    "        if Dosing_Speed > 1500 or Dosing_Speed < 450:\n",
    "            reward_5 = -30\n",
    "            working_range = 1\n",
    "\n",
    "        if working_range == 1:\n",
    "            new_state_1 = [Silo_Filllevel_1, Hopper_Filllevel_1, MF_VC_2, Demand, 0]\n",
    "            new_state_2 = [Silo_Filllevel_2, Hopper_Filllevel_2, MF_VC_2, MF_VC_3, Demand]\n",
    "            new_state_3 = [Silo_Filllevel_3, Hopper_Filllevel_3, MF_VC_2, MF_VC_3, Demand]\n",
    "            new_state_space_1 = new_state_1\n",
    "            new_state_space_2 = new_state_2\n",
    "            new_state_space_3 = new_state_3\n",
    "\n",
    "        else:\n",
    "            new_state = environment_bgs()\n",
    "         \n",
    "            new_Silo_Filllevel_1 = new_state[0]\n",
    "            new_Silo_Filllevel_2 = new_state[1]\n",
    "            new_Silo_Filllevel_3 = new_state[2]\n",
    "            new_Hopper_Filllevel_1 = new_state[3]\n",
    "            new_Hopper_Filllevel_2 = new_state[4]\n",
    "            new_Hopper_Filllevel_3 = new_state[5]\n",
    "            new_MF_VC_2 = new_state[16]\n",
    "            new_MF_VC_3 = new_state[17]\n",
    "            Total_Energy_Sim = new_state[40]\n",
    "            print('hi')\n",
    "            \n",
    "            new_state_1 = [new_Silo_Filllevel_1, new_Hopper_Filllevel_1, new_MF_VC_2, new_Demand, 0]\n",
    "            new_state_2 = [new_Silo_Filllevel_2, new_Hopper_Filllevel_2, new_MF_VC_2, new_MF_VC_3, new_Demand]\n",
    "            new_state_3 = [new_Silo_Filllevel_3, new_Hopper_Filllevel_3, new_MF_VC_2, new_MF_VC_3, new_Demand]\n",
    "            new_state_space_1 = torch.tensor(new_state_1)\n",
    "            new_state_space_2 = torch.tensor(new_state_2)\n",
    "            new_state_space_3 = torch.tensor(new_state_3)\n",
    "        \n",
    "            # push this exp in ram\n",
    "        ram_1.add(state_space_1.tolist(), Conveyor_Motor_Speed, reward_1, new_state_space_1)\n",
    "        ram_2.add(state_space_2.tolist(), [VP_Time_2], reward_2, new_state_space_2)\n",
    "        ram_3.add(state_space_2.tolist(), Vibration_Belt_Start, reward_3, new_state_space_2)\n",
    "        ram_4.add(state_space_3.tolist(), VP_Time_3, reward_4, new_state_space_3)\n",
    "        ram_5.add(state_space_3.tolist(), Dosing_Speed, reward_5, new_state_space_3)\n",
    "        \n",
    "        \n",
    "            # perform optimization\n",
    "        trainer_1.optimize()\n",
    "        trainer_2.optimize()\n",
    "        trainer_3.optimize()\n",
    "        trainer_4.optimize()\n",
    "        trainer_5.optimize()\n",
    "        \n",
    "        #C = trainer_1.A\n",
    "        #D = trainer_1.B\n",
    "        #E = trainer_2.A\n",
    "        #F = trainer_2.B\n",
    "        #G = trainer_3.A\n",
    "        #H = trainer_3.B\n",
    "        #I = trainer_4.A\n",
    "        #J = trainer_4.B\n",
    "        #K = trainer_5.A\n",
    "        #L = trainer_5.B\n",
    "        \n",
    "        #sum_of_actor_loss_1 = sum_of_actor_loss + C\n",
    "        #sum_of_critic_loss_1 = sum_of_critic_loss + D\n",
    "        #sum_of_actor_loss_2 = sum_of_actor_loss + E\n",
    "        #sum_of_critic_loss_2 = sum_of_critic_loss +  F\n",
    "        #sum_of_actor_loss_3 = sum_of_actor_loss + G \n",
    "        #sum_of_critic_loss_3 = sum_of_critic_loss + H \n",
    "        #sum_of_actor_loss_4 = sum_of_actor_loss + I\n",
    "        #sum_of_critic_loss_4 = sum_of_critic_loss + J \n",
    "        #sum_of_actor_loss_5 = sum_of_actor_loss + K\n",
    "        #sum_of_critic_loss_5 = sum_of_critic_loss + L \n",
    "        \n",
    "        if reached == 1:\n",
    "            break\n",
    "            \n",
    "        ############## End of Episode ################\n",
    "        \n",
    "    Energy_Consumed_Per_Episode.append(Total_Energy_Sim)\n",
    "    \n",
    "    Reward_Per_Episode_1.append(Total_Reward_1)\n",
    "    Reward_Per_Episode_2.append(Total_Reward_2)\n",
    "    Reward_Per_Episode_3.append(Total_Reward_3)\n",
    "    Reward_Per_Episode_4.append(Total_Reward_4)\n",
    "    Reward_Per_Episode_5.append(Total_Reward_5)\n",
    "    \n",
    "    #Actor_loss_per_episode_1.append(sum_of_actor_loss_1)\n",
    "    #Critic_loss_per_episode_1.append(sum_of_critic_loss_1)\n",
    "    #Actor_loss_per_episode_2.append(sum_of_actor_loss_2)\n",
    "    #Critic_loss_per_episode_2.append(sum_of_critic_loss_2)\n",
    "    #Actor_loss_per_episode_3.append(sum_of_actor_loss_3)\n",
    "    #Critic_loss_per_episode_3.append(sum_of_critic_loss_3)\n",
    "    #Actor_loss_per_episode_4.append(sum_of_actor_loss_4)\n",
    "    #Critic_loss_per_episode_4.append(sum_of_critic_loss_4)\n",
    "    #Actor_loss_per_episode_5.append(sum_of_actor_loss_5)\n",
    "    #Critic_loss_per_episode_5.append(sum_of_critic_loss_5)\n",
    "    \n",
    "    # check memory consumption and clear memory\n",
    "    gc.collect()\n",
    "    #print('Episode: ', _ep)\n",
    "    # process = psutil.Process(os.getpid())\n",
    "    # print(process.memory_info().rss)\n",
    "    torch.save(Energy_Consumed_Per_Episode, 'Energy_Consumed_Per_Episode')\n",
    "    torch.save(Reward_Per_Episode_1, 'Reward_Per_Episode_1')\n",
    "    torch.save(Reward_Per_Episode_2, 'Reward_Per_Episode_2')\n",
    "    torch.save(Reward_Per_Episode_3, 'Reward_Per_Episode_3')\n",
    "    torch.save(Reward_Per_Episode_4, 'Reward_Per_Episode_4')\n",
    "    torch.save(Reward_Per_Episode_5, 'Reward_Per_Episode_5')\n",
    "    #torch.save(Actor_loss_per_episode_1, 'Actor_loss_per_episode_1')\n",
    "    #torch.save(Critic_loss_per_episode_1, 'Critic_loss_per_episode_1')\n",
    "    #torch.save(Actor_loss_per_episode_2, 'Actor_loss_per_episode_2')\n",
    "    #torch.save(Critic_loss_per_episode_2, 'Critic_loss_per_episode_2')\n",
    "    #torch.save(Actor_loss_per_episode_3, 'Actor_loss_per_episode_3')\n",
    "    #torch.save(Critic_loss_per_episode_3, 'Critic_loss_per_episode_3')\n",
    "    #torch.save(Actor_loss_per_episode_4, 'Actor_loss_per_episode_4')\n",
    "    #torch.save(Critic_loss_per_episode_4, 'Critic_loss_per_episode_4')\n",
    "    #torch.save(Actor_loss_per_episode_5, 'Actor_loss_per_episode_5')\n",
    "    #torch.save(Critic_loss_per_episode_5, 'Critic_loss_per_episode_5')\n",
    "    \n",
    "    \n",
    "    if _ep%10 == 0:        \n",
    "        print(_ep)\n",
    "        trainer_1.save_models(_ep, r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_actor_param_1.pt\", r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_critic_param_1.pt\" )\n",
    "        trainer_2.save_models(_ep, r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_actor_param_2.pt\", r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_critic_param_2.pt\")\n",
    "        trainer_3.save_models(_ep, r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_actor_param_3.pt\", r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_critic_param_3.pt\")\n",
    "        trainer_4.save_models(_ep, r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_actor_param_4.pt\", r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_critic_param_4.pt\")\n",
    "        trainer_5.save_models(_ep, r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_actor_param_5.pt\", r\"C:\\Users\\Madhav\\Desktop\\Python DDPG\\target_critic_param_5.pt\")\n",
    "    # C:\\Users\\hp\\Desktop\\Python DDPG\n",
    "    #torch.save(trainer_1.Actor.state_dict(), 'model_1.pt')\n",
    "    #model_critic = \n",
    "print ('\\n \\n Completed episodes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward_Per_Episode_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Reward_Per_Episode_1, Reward_Per_Episode_2, Reward_Per_Episode_3, Reward_Per_Episode_4, Reward_Per_Episode_5)\n",
    "Energy_Consumed_Per_Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(r\"C:\\Users\\hp\\Desktop\\Python DDPG\\target_critic_param_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conveyor_Motor_Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VP_Time_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
